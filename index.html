---
layout: default
title: "Home"
description: "Core Knowledge Deficits in Multi-Modal Language Models"
---

<!-- Hero Section -->
<section class="gradient-bg text-slate-700 py-12">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
        <h1 class="text-3xl md:text-4xl font-bold text-slate-800 mb-8 max-w-4xl mx-auto">
            Core Knowledge Deficits in Multi-Modal Language Models
        </h1>
        
        <!-- Simplified TLDR with cleaner formatting -->
        <div class="text-lg md:text-xl text-slate-700 max-w-3xl mx-auto leading-relaxed space-y-4 mb-10">
            <p>
                While Multi-modal Large Language Models (MLLMs) demonstrate impressive abilities over high-level perception and reasoning, their robustness in the wild remains limited, often falling short on tasks that are intuitive and effortless for humans. We examine the hypothesis that these deficiencies stem from the absence of core knowledge—rudimentary cognitive abilities innate to humans from early childhood. 
            </p>
            <p>
                To explore the core knowledge representation in MLLMs, we introduce <span class="font-semibold text-blue-700">CoreCognition</span>, a large-scale benchmark encompassing 12 core knowledge concepts grounded in developmental cognitive science.
                We evaluate <span class="font-semibold text-slate-900">230 models with 11 different prompts</span>, leading to a total of <span class="font-semibold text-slate-900">2,530 data points</span> for analysis. Our experiments uncover four key findings, collectively demonstrating core knowledge deficits in MLLMs: they consistently underperform and show reduced, or even absent, scalability on low-level abilities relative to high-level ones.
            </p>
            <p>
                Finally, we propose <span class="font-semibold text-blue-700">Concept Hacking</span>, a novel controlled evaluation method, that reveals MLLMs fail to progress toward genuine core knowledge understanding, but instead rely on shortcut learning as they scale.
            </p>
        </div>
        
        <!-- Updated CTA Buttons -->
        <div class="flex flex-col sm:flex-row gap-3 justify-center items-center mb-10">
            <a href="{{ site.paper_url | default: '#' }}" 
               class="inline-flex items-center bg-white text-slate-700 px-6 py-2.5 rounded-lg font-semibold hover:bg-slate-50 transition-all transform hover:scale-105 shadow-lg border border-slate-200">
                📄 <span class="ml-2">Paper</span>
            </a>
            <a href="{{ site.huggingface_url | default: '#' }}" 
               class="inline-flex items-center bg-gradient-to-r from-yellow-400 to-orange-500 text-white px-6 py-2.5 rounded-lg font-semibold hover:from-yellow-500 hover:to-orange-600 transition-all transform hover:scale-105 shadow-lg">
                🤗 <span class="ml-2">Dataset</span>
            </a>
        </div>
    </div>
</section>

<!-- Authors Section -->
<section class="py-10 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
        <div class="text-base md:text-lg text-gray-700 mb-6">
            <div class="space-y-3">
                <!-- First line of authors (5 authors) -->
            <div class="flex flex-wrap justify-center gap-x-6 gap-y-2">
                <span>Yijiang Li<sup>1</sup></span>
                <span>Qingying Gao<sup>2,§</sup></span>
                <span>Tianwei Zhao<sup>2,§</sup></span>
                <span>Bingyang Wang<sup>3,§</sup></span>
                <span>Haoran Sun<sup>2</sup></span>
                </div>
                <!-- Second line of authors (5 authors) -->
                <div class="flex flex-wrap justify-center gap-x-6 gap-y-2">
                <span>Haiyun Lyu<sup>4</sup></span>
                <span>Robert D. Hawkins<sup>5</sup></span>
                <span>Nuno Vasconcelos<sup>1</sup></span>
                <span>Tal Golan<sup>6</sup></span>
                <span>Dezhi Luo<sup>7,8,†</sup></span>
                <span>Hokin Deng<sup>9,†</sup></span>
            </div>
        </div>
        </div>
        <div class="text-base md:text-lg text-gray-600">
            <div class="mb-4 font-bold">
                <sup>1</sup>University of California San Diego &emsp;
                <sup>2</sup>Johns Hopkins University &emsp;
                <sup>3</sup>Emory University &emsp;
                <sup>4</sup>University of North Carolina at Chapel Hill &emsp;
                <sup>5</sup>Stanford University &emsp;
            </div>
            <div class="mb-4 font-bold">
                <sup>6</sup>Ben-Gurion University of the Negev &emsp;
                <sup>7</sup>University of Michigan &emsp;
                <sup>8</sup>University College London &emsp;
                <sup>9</sup>Carnegie Mellon University &emsp;
            </div>
            <p class="text-sm md:text-base text-gray-500">
                <sup>§</sup>Equal Contribution &emsp; 
                <sup>†</sup>Corresponding author
            </p>
        </div>
    </div>
</section>

<!-- Problem Demonstration with Real Cases -->
<section class="py-20 bg-gradient-to-br from-red-50 to-orange-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">The Core Problem: Flawed Training & Narrow Evaluation</h2>
            <p class="text-xl text-gray-600 max-w-4xl mx-auto">
                Current reward models face fundamental limitations in how they are <span class="font-semibold text-red-700">trained</span> and <span class="font-semibold text-red-700">evaluated</span>, hindering their ability to truly align with diverse human values.
            </p>
        </div>

        <!-- Problematic Training -->
        <div class="mb-12 bg-white rounded-2xl p-8 shadow-lg border border-red-100">
            <h3 class="text-2xl font-bold text-gray-900 mb-4 text-center">1. Problematic Training: Learning Static & Biased Preferences</h3>
            <p class="text-gray-700 mb-4 text-center max-w-3xl mx-auto">
                Reward models are typically trained on vast datasets of <span class="font-semibold text-red-700">(prompt, chosen response, rejected response)</span> tuples. This teaches the model a <span class="font-semibold text-red-700">single, implicit preference</span> distribution.
            </p>
            <div class="grid md:grid-cols-2 gap-6 text-sm text-gray-600">
                <div class="bg-red-50 p-4 rounded-lg">
                    <h4 class="font-semibold text-red-800 mb-1">No Principled Control:</h4>
                    <p>Even if the prompt and responses are identical, applying <span class="italic">different evaluation principles</span> (e.g., "be concise" vs. "be detailed") should lead to different rankings. Current RMs struggle to adapt this way without costly retraining for each new principle.</p>
                </div>
                <div class="bg-red-50 p-4 rounded-lg">
                    <h4 class="font-semibold text-red-800 mb-1">Implicit & Outcome-Only Learning:</h4>
                    <p>Models learn <span class="italic">what</span> to prefer based on outcomes, but not <span class="italic">why</span>. This lack of explicit rationale leads to learning superficial patterns or <span class="font-semibold text-red-700">spurious correlations</span> (e.g., "longer is better," "lists are good") rather than the true underlying human intent, as shown below.</p>
                </div>
            </div>
        </div>

        <!-- Problem Illustration with Real Cases -->
        <div class="grid lg:grid-cols-2 gap-12 mb-16">
            <!-- Case 1: Length Bias -->
            <div class="bg-white rounded-2xl p-8 shadow-lg border border-red-100">
                <div class="flex items-center mb-6">
                    <div class="bg-red-100 p-3 rounded-lg mr-4">
                        <svg class="w-6 h-6 text-red-600" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7 4a1 1 0 11-2 0 1 1 0 012 0zm-1-9a1 1 0 00-1 1v4a1 1 0 102 0V6a1 1 0 00-1-1z" clip-rule="evenodd"/>
                        </svg>
                    </div>
                    <div>
                        <h3 class="text-xl font-bold text-gray-900">Issue #1: Length = Quality Bias</h3>
                        <p class="text-sm text-gray-600">Models learn "longer responses are better" from pairs where correctness correlates with length.</p>
                    </div>
                </div>
                
                <div class="space-y-4">
                    <div class="bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400">
                        <div class="text-sm font-semibold text-blue-900 mb-2">🙋 Prompt:</div>
                        <p class="text-blue-800 text-sm">"What are some species of bears that are now extinct?"</p>
                    </div>
                    
                    <div class="grid md:grid-cols-2 gap-4">
                        <div class="bg-green-50 p-4 rounded-lg border border-green-200">
                            <div class="flex items-center mb-2">
                                <span class="text-green-600 font-semibold text-sm mr-2">✅ Chosen (Long + Correct)</span>
                            </div>
                            <p class="text-green-800 text-xs leading-relaxed">
                                "Several species of bears have become extinct... <strong>Cave Bear (Ursus spelaeus)</strong>: One of the best-known extinct bear species... <strong>Short-faced Bear (Arctodus simus)</strong>: Once the largest..."
                            </p>
                            <div class="mt-2 text-xs text-green-600">
                                ✓ Accurate facts ✓ Detailed explanations
                            </div>
                        </div>
                        
                        <div class="bg-red-50 p-4 rounded-lg border border-red-200">
                            <div class="flex items-center mb-2">
                                <span class="text-red-600 font-semibold text-sm mr-2">❌ Rejected (Short + Wrong)</span>
                            </div>
                            <p class="text-red-800 text-xs leading-relaxed">
                                "Three species of bears that are now extinct are the <strong>woolly mammoth</strong>, the <strong>woolly rhinoceros</strong>, and the <strong>thylacine</strong>."
                            </p>
                            <div class="mt-2 text-xs text-red-600">
                                ❌ Factually incorrect ❌ None are bears
                            </div>
                        </div>
                    </div>
                    
                    <div class="bg-yellow-50 p-4 rounded-lg border border-yellow-200">
                        <div class="text-sm font-semibold text-yellow-800 mb-1">⚠️ What Current RMs Learn:</div>
                        <p class="text-yellow-700 text-sm">A spurious correlation: "Longer responses are better." This preference is static, but what if the user actually preferred a brief, accurate answer?</p>
                    </div>
                </div>
            </div>

            <!-- Case 2: Format Over Substance -->
            <div class="bg-white rounded-2xl p-8 shadow-lg border border-red-100">
                <div class="flex items-center mb-6">
                    <div class="bg-red-100 p-3 rounded-lg mr-4">
                        <svg class="w-6 h-6 text-red-600" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7 4a1 1 0 11-2 0 1 1 0 012 0zm-1-9a1 1 0 00-1 1v4a1 1 0 102 0V6a1 1 0 00-1-1z" clip-rule="evenodd"/>
                        </svg>
                    </div>
                    <div>
                        <h3 class="text-xl font-bold text-gray-900">Issue #2: Format Over Substance</h3>
                        <p class="text-sm text-gray-600">Models often prioritize familiar structures (e.g., lists) over equally valid, natural content.</p>
                    </div>
                </div>
                
                <div class="space-y-4">
                    <div class="bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400">
                        <div class="text-sm font-semibold text-blue-900 mb-2">🙋 Prompt:</div>
                        <p class="text-blue-800 text-sm">"What are some good browser alternatives to Chrome?"</p>
                    </div>
                    
                    <div class="grid md:grid-cols-2 gap-4">
                        <div class="bg-green-50 p-4 rounded-lg border border-green-200">
                            <div class="flex items-center mb-2">
                                <span class="text-green-600 font-semibold text-sm mr-2">✅ Chosen (Well-Structured)</span>
                            </div>
                            <p class="text-green-800 text-xs leading-relaxed">
                                "There are several good browser alternatives to Chrome:
                                <br><strong>1. Mozilla Firefox:</strong> Known for strong privacy features, being open-source, and highly customizable.
                                <br><strong>2. Microsoft Edge:</strong> Now built on Chromium, offering good performance and compatibility."
                            </p>
                            <div class="mt-2 text-xs text-green-600">
                                ✓ Clear, itemized structure ✓ Detailed points
                            </div>
                        </div>
                        
                        <div class="bg-orange-50 p-4 rounded-lg border border-orange-200">
                            <div class="flex items-center mb-2">
                                <span class="text-orange-600 font-semibold text-sm mr-2">⚠️ Rejected (Natural but Correct)</span>
                            </div>
                            <p class="text-orange-800 text-xs leading-relaxed">
                                "Sure! For browser alternatives, you could check out Firefox – it's really good for privacy and you can customize it a lot. Microsoft Edge is another option; it's pretty fast now that it uses Chromium tech."
                            </p>
                            <div class="mt-2 text-xs text-orange-600">
                                ✓ Factually correct ✓ Conversational style ✓ Complete
                            </div>
                        </div>
                    </div>
                    
                    <div class="bg-yellow-50 p-4 rounded-lg border border-yellow-200">
                        <div class="text-sm font-semibold text-yellow-800 mb-1">⚠️ What Current RMs Learn:</div>
                        <p class="text-yellow-700 text-sm">"Structured, list-like responses are better." This overlooks that a natural, conversational style might be equally informative or even preferred by some users.</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Incomplete Evaluation -->
        <div class="my-16 bg-white rounded-2xl p-8 shadow-lg border border-orange-100">
            <h3 class="text-2xl font-bold text-gray-900 mb-4 text-center">2. Incomplete Evaluation: Missing True Generalization</h3>
            <p class="text-gray-700 mb-4 text-center max-w-3xl mx-auto">
                Existing Reward Model benchmarks primarily measure how well an RM aligns with a <span class="font-semibold text-orange-700">single, predefined preference distribution</span> (often the one it was trained on or a similar one).
            </p>
            <div class="grid md:grid-cols-2 gap-6 text-sm text-gray-600">
                <div class="bg-orange-50 p-4 rounded-lg">
                    <h4 class="font-semibold text-orange-800 mb-1">Ignoring Multifaceted Values:</h4>
                    <p>Human preferences are complex, context-dependent, and multifaceted. A truly useful RM must adapt to <span class="italic">any</span> explicitly stated principle, not just echo a single, baked-in preference.</p>
                </div>
                <div class="bg-orange-50 p-4 rounded-lg">
                    <h4 class="font-semibold text-orange-800 mb-1">Superficial Alignment:</h4>
                    <p>This narrow evaluation fails to assess the critical capability of <span class="font-semibold text-orange-700">generalizing to diverse and novel principles</span> at inference time, which is essential for robust and trustworthy AI systems.</p>
                </div>
            </div>
        </div>

        <!-- The Core Problems -->
        <div class="bg-white rounded-2xl p-8 shadow-lg border border-gray-200">
            <h3 class="text-2xl font-bold text-gray-900 mb-2 text-center">Consequences: Why Current Reward Models Fall Short</h3>
            <p class="text-gray-600 text-center mb-8 max-w-3xl mx-auto">These fundamental issues in training and evaluation lead to several critical shortcomings:</p>
            
            <div class="grid md:grid-cols-2 lg:grid-cols-4 gap-6">
                <div class="text-center">
                    <div class="bg-red-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <!-- Icon for Overfitting to Static Preferences -->
                        <svg class="w-8 h-8 text-red-600" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M5 9V7a5 5 0 0110 0v2a2 2 0 012 2v5a2 2 0 01-2 2H5a2 2 0 01-2-2v-5a2 2 0 012-2zm5-3a3 3 0 00-3 3v2h6V7a3 3 0 00-3-3z" clip-rule="evenodd" />
                        </svg>
                    </div>
                    <h4 class="font-semibold text-gray-900 mb-2">Overfitting to Static Preferences</h4>
                    <p class="text-sm text-gray-600">RMs master a single, fixed preference from training data, failing to grasp the multifaceted nature of human values or adapt to diverse contexts.</p>
                </div>
                
                <div class="text-center">
                    <div class="bg-red-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <!-- Icon for Opaque & Implicit Reasoning -->
                        <svg class="w-8 h-8 text-red-600" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-8-3a1 1 0 00-.867.5 1 1 0 11-1.731-1A3 3 0 0113 8a3.001 3.001 0 01-2 2.83V11a1 1 0 11-2 0v-1a1 1 0 011-1 1 1 0 100-2zm0 8a1 1 0 100-2 1 1 0 000 2z" clip-rule="evenodd" />
                        </svg>
                    </div>
                    <h4 class="font-semibold text-gray-900 mb-2">Opaque & Implicit Reasoning</h4>
                    <p class="text-sm text-gray-600">Learning from outcomes alone (chosen/rejected pairs), RMs lack an explicit understanding of *why* a response is preferred, making their judgments uninterpretable black boxes.</p>
                </div>
                
                <div class="text-center">
                    <div class="bg-red-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <!-- Icon for Vulnerability to Spurious Correlations -->
                        <svg class="w-8 h-8 text-red-600" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M8.485 2.495c.673-1.167 2.357-1.167 3.03 0l6.28 10.875c.673 1.167-.17 2.625-1.516 2.625H3.72c-1.347 0-2.189-1.458-1.515-2.625L8.485 2.495zM10 5a.75.75 0 01.75.75v3.5a.75.75 0 01-1.5 0v-3.5A.75.75 0 0110 5zm0 9a1 1 0 100-2 1 1 0 000 2z" clip-rule="evenodd" />
                        </svg>
                    </div>
                    <h4 class="font-semibold text-gray-900 mb-2">Vulnerability to Spurious Correlations</h4>
                    <p class="text-sm text-gray-600">Implicit learning on biased data leads RMs to mistakenly learn superficial cues (e.g., length, format, specific keywords) as proxies for genuine quality.</p>
                </div>
                
                <div class="text-center">
                    <div class="bg-red-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <!-- Icon for Costly & Inefficient Adaptation -->
                        <svg class="w-8 h-8 text-red-600" fill="currentColor" viewBox="0 0 20 20">
                           <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm.75-13a.75.75 0 00-1.5 0v5c0 .414.336.75.75.75h4a.75.75 0 000-1.5h-3.25V5z" clip-rule="evenodd" />
                        </svg>
                    </div>
                    <h4 class="font-semibold text-gray-900 mb-2">Costly & Inefficient Adaptation</h4>
                    <p class="text-sm text-gray-600">Due to overfit, static preferences and opaque reasoning, aligning RMs with new criteria or principles demands expensive data collection and full retraining cycles.</p>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- New Solution Section -->
<section class="py-20 bg-slate-50"> <!-- Neutral background for the solution section -->
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">The Solution: Principle-Following Reward Models</h2>
            <p class="text-xl text-gray-600 max-w-4xl mx-auto">
                To overcome these limitations, we propose a paradigm shift towards reward models that explicitly understand and follow natural language principles. This approach enables dynamic adaptation to any evaluation criteria without costly retraining and is embodied by two key innovations:
            </p>
        </div>

        <div class="grid md:grid-cols-1 lg:grid-cols-2 gap-10 mb-16 items-stretch">
            <!-- Sub-block 1: Evaluating Principle-Following (RABench) -->
            <div class="bg-white p-8 rounded-xl shadow-lg border border-gray-200 flex flex-col">
                <div class="flex items-center mb-4">
                    <span class="text-3xl mr-3">🎯</span>
                    <h3 class="text-2xl font-bold text-blue-700">1. A New Evaluation Paradigm: RABench</h3>
                </div>
                <p class="text-gray-700 mb-4 flex-grow">
                    Current benchmarks assess how well RMs fit a <span class="font-semibold">single, fixed preference</span>. This is insufficient. We argue that, analogous to how Large Language Models (LLMs) are valued for their ability to <span class="font-semibold text-blue-600">follow diverse instructions</span>, reward models must be evaluated on their capacity to <span class="font-semibold text-blue-600">follow diverse principles</span>.
                </p>
                <p class="text-gray-700 mb-4 flex-grow">
                    To this end, we introduce <strong class="text-blue-700">RABench (RewardAnything Benchmark)</strong>. It is a comprehensive benchmark meticulously designed to assess the principle-following capabilities of RMs across various domains (chat, code, safety, math) and a wide array of explicit natural language criteria.
                </p>
                 <p class="text-gray-700">
                    RABench moves beyond static preference matching, pushing for RMs that demonstrate true generalization in understanding and applying "goodness" based on varied, explicit guidance.
                </p>
            </div>

            <!-- Sub-block 2: RewardAnything Model -->
            <div class="bg-white p-8 rounded-xl shadow-lg border border-gray-200 flex flex-col">
                 <div class="flex items-center mb-4">
                    <span class="text-3xl mr-3">🏆</span>
                    <h3 class="text-2xl font-bold text-green-700">2. The RewardAnything Model</h3>
                </div>
                <p class="text-gray-700 mb-4 flex-grow">
                    We develop <strong class="text-green-700">RewardAnything</strong>, a novel reward model engineered to embody this principle-following paradigm.
                </p>
                <p class="text-gray-700 mb-4 flex-grow">
                    Trained using advanced Reinforcement Learning (RL) techniques on principle-conditioned preference data, RewardAnything learns to robustly distinguish better responses from worse ones by <span class="font-semibold text-green-600">directly conditioning on explicit natural language principles</span> provided at inference time. This allows it to adapt its judgment dynamically without any retraining.
                </p>
                <p class="text-gray-700">
                    A key feature is its <span class="font-semibold text-green-600">inference-time reasoning process</span>. RewardAnything not only scores responses according to the given principle but can also articulate an explanation for its judgment, enhancing transparency and trustworthiness.
                </p>
            </div>
        </div>

        <!-- Figure 1 Moved Here -->
        <div class="paper-figure-container mb-0"> <!-- paper-figure-container already has good styling -->
            <div class="max-w-5xl mx-auto">
                <img src="{{ '/assets/images/figure_1_placeholder.jpg' | relative_url }}" 
                     alt="Figure 1: Current post-training optimization paradigm vs RewardAnything approach"
                     class="w-full h-auto rounded-lg shadow-sm">
            </div>
        </div>
    </div>
</section>

<!-- Refer to Paper for Technical Details -->
<section class="py-16 bg-slate-100">
    <div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
        <h2 class="text-2xl md:text-3xl font-semibold text-gray-800 mb-6">
            <span class="mr-2">📖</span>Dive Deeper into the Details
        </h2>
        <p class="text-lg md:text-xl text-gray-700 leading-relaxed mb-8">
            For a comprehensive understanding of our methodology, technical innovations, detailed model architecture, training procedures, and full experimental setup, please refer to our full research paper. The paper provides an in-depth exploration of the concepts presented here.
        </p>
        <a href="{{ site.paper_url | default: '#' }}" 
           class="inline-flex items-center bg-blue-600 text-white px-8 py-3 rounded-lg font-semibold hover:bg-blue-700 transition-all transform hover:scale-105 shadow-lg text-base md:text-lg">
            📄 <span class="ml-2">Read the Full Paper</span>
        </a>
    </div>
</section>

<!-- Installation & Usage Guide -->
<section id="leaderboard" class="py-20 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">🚀 LeaderBoard</h2>
            <p class="text-xl text-gray-600 max-w-4xl mx-auto">
                Core Cognitive Benchmarking ... Coming Soon!
            </p>
        </div>

        <!-- LeaderBoard -->
        <!-- Scrollable Table Container -->
    <div class="overflow-x-auto max-w-full border border-gray-200 rounded-lg">
      <table class="min-w-full table-auto text-sm text-left text-gray-600">
        <thead class="bg-gray-100">
          <tr>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Model Name</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Boundary</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Continuity</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Permanence</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Spatiality</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Perceptual Constancy</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Intuitive Physics</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Perspective</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Conservation</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Hierarchy</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Intentionality</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Mechanical Reasoning</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Tool Using</th>
            <th class="sticky top-0 z-10 bg-gray-100 px-4 py-2 font-semibold text-gray-900">Mean</th>
          </tr>
        </thead>
        <tbody>
          {% for row in site.data.leaderboard %}
          <tr class="border-t border-gray-200">
            <td class="px-4 py-2 font-medium text-gray-900">{{ row.model_name }}</td>
            {% assign fields = "boundary,continuity,permanence,spatiality,perceptualconstancy,intuitivephysics,perspective,conservation,hierarchy,intentionality,mechanicalreasoning,toolusing,mean" | split: "," %}
            {% for field in fields %}
              {% assign value = row[field] | plus: 0 %}
              <td class="px-4 py-2 {% if value >= 90 %}bg-green-100 font-semibold{% endif %}">{{ value | round: 2 }}%</td>
            {% endfor %}
          </tr>
          {% endfor %}
        </tbody>
      </table>
    </div>

    </div>
</section>


<!-- Performance Results -->
<section class="py-20 bg-gradient-to-br from-blue-50 to-indigo-100">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">State-of-the-Art Performance</h2>
            <p class="text-xl text-gray-600 max-w-4xl mx-auto">
                RewardAnything achieves excellent performance on both traditional benchmarks and our new principle-following evaluation. Below are highlights from RM-Bench and our proposed RABench. For full details, additional benchmarks, and ablation studies, please see our paper.
            </p>
        </div>

        <div class="space-y-12 md:space-y-16">
            <div class="bg-white p-4 md:p-6 rounded-xl shadow-xl border border-gray-200">
                <h3 class="text-xl md:text-2xl font-semibold text-gray-800 mb-4 text-center">Table 2: Performance on RM-Bench</h3>
                <img src="{{ '/assets/images/table2_rm_bench_results.jpg' | relative_url }}" 
                     alt="Table 2: Accuracies (%) of reward models on RM-Bench"
                     class="w-full h-auto rounded-md shadow-md mx-auto border border-gray-300" style="max-width: 800px;">
            </div>

            <div class="bg-white p-4 md:p-6 rounded-xl shadow-xl border border-gray-200">
                <h3 class="text-xl md:text-2xl font-semibold text-gray-800 mb-4 text-center">Table 3: Performance on RABench (Ours)</h3>
                <img src="{{ '/assets/images/table3_rabench_results.jpg' | relative_url }}" 
                     alt="Table 3: Performance of reward models on RABench"
                     class="w-full h-auto rounded-md shadow-md mx-auto border border-gray-300" style="max-width: 900px;">
            </div>
        </div>
    </div>
</section>

<!-- Key Features -->
<section class="py-20 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">Key Innovations</h2>
            <p class="text-xl text-gray-600 max-w-3xl mx-auto">
                RewardAnything introduces novel techniques for principle-following reward modeling
            </p>
        </div>

        <div class="grid grid-cols-1 md:grid-cols-2 gap-12">
            <div class="space-y-8">
                <div class="space-y-4">
                    <div class="flex items-start">
                        <div class="bg-blue-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-blue-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Group Relative Policy Optimization (GRPO)</h4>
                            <p class="text-gray-600">Advanced RL training that learns relative preferences within response groups</p>
                        </div>
                    </div>
                    <div class="flex items-start">
                        <div class="bg-blue-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-blue-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Listwise Evaluation</h4>
                            <p class="text-gray-600">Efficient ranking of multiple responses in a single forward pass</p>
                        </div>
                    </div>
                    <div class="flex items-start">
                        <div class="bg-blue-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-blue-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Inference-Time Reasoning</h4>
                            <p class="text-gray-600">Explicit reasoning process for transparent decision making</p>
                        </div>
                    </div>
                </div>
                <div class="space-y-4">
                    <div class="flex items-start">
                        <div class="bg-green-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-green-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Multi-LLM Consensus</h4>
                            <p class="text-gray-600">Ground truth from 4 state-of-the-art LLMs with algorithmic consensus</p>
                        </div>
                    </div>
                    <div class="flex items-start">
                        <div class="bg-green-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-green-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Human Verification</h4>
                            <p class="text-gray-600">89% agreement rate with κ=0.57 for reliable evaluation standards</p>
                        </div>
                    </div>
                </div>
            </div>

            <!-- RABench Description -->
            <div class="bg-gray-50 p-8 rounded-xl">
                <h3 class="text-xl font-bold text-gray-900 mb-4">RABench: Novel Evaluation Framework</h3>
                <p class="text-gray-700 mb-6">
                    We introduce RABench, a comprehensive benchmark specifically designed to evaluate reward models' 
                    ability to follow explicit natural language principles across diverse domains and criteria.
                </p>
                <div class="space-y-4">
                    <div class="flex items-center">
                        <div class="w-3 h-3 bg-blue-500 rounded-full mr-3"></div>
                        <span class="text-gray-700"><strong>1,002 validated rankings</strong> across 50 principles</span>
                    </div>
                    <div class="flex items-center">
                        <div class="w-3 h-3 bg-green-500 rounded-full mr-3"></div>
                        <span class="text-gray-700"><strong>5 principle categories:</strong> Content, Logic, Style, Tone, Structure</span>
                    </div>
                    <div class="flex items-center">
                        <div class="w-3 h-3 bg-purple-500 rounded-full mr-3"></div>
                        <span class="text-gray-700"><strong>Multiple domains:</strong> Chat, Code, Safety, Math</span>
                    </div>
                    <div class="flex items-center">
                        <div class="w-3 h-3 bg-orange-500 rounded-full mr-3"></div>
                        <span class="text-gray-700"><strong>Human-verified quality</strong> with high inter-annotator agreement</span>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Documentation -->
<section id="documentation" class="py-20 bg-gray-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">Documentation & Resources</h2>
            <p class="text-xl text-gray-600 max-w-3xl mx-auto">
                Everything you need to understand and use RewardAnything for your research and applications
            </p>
        </div>

        <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6">
            <a href="{{ site.paper_url | default: '#' }}" class="block bg-blue-50 p-6 rounded-xl hover:bg-blue-100 transition-colors">
                <div class="text-2xl mb-3">📄</div>
                <h3 class="font-semibold text-blue-900 mb-2">Research Paper</h3>
                <p class="text-sm text-blue-700">Complete methodology, experiments, and theoretical foundations</p>
            </a>

            <a href="#" class="block bg-green-50 p-6 rounded-xl hover:bg-green-100 transition-colors">
                <div class="text-2xl mb-3">🚀</div>
                <h3 class="font-semibold text-green-900 mb-2">API Documentation</h3>
                <p class="text-sm text-green-700">Comprehensive guide to using RewardAnything in your code</p>
            </a>

            <a href="#" class="block bg-purple-50 p-6 rounded-xl hover:bg-purple-100 transition-colors">
                <div class="text-2xl mb-3">📊</div>
                <h3 class="font-semibold text-purple-900 mb-2">RABench Dataset</h3>
                <p class="text-sm text-purple-700">Benchmark dataset for evaluating principle-following capabilities</p>
            </a>

            <a href="{{ site.huggingface_url | default: '#' }}" class="block bg-orange-50 p-6 rounded-xl hover:bg-orange-100 transition-colors">
                <div class="text-2xl mb-3">🤗</div>
                <h3 class="font-semibold text-orange-900 mb-2">Model Weights</h3>
                <p class="text-sm text-orange-700">Pre-trained models ready for inference and fine-tuning</p>
            </a>
        </div>
    </div>
</section>

<!-- Citation -->
<section class="py-20 bg-gray-900 text-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-12">
            <h2 class="text-3xl md:text-4xl font-bold mb-4">Citation</h2>
            <p class="text-xl text-gray-300 max-w-3xl mx-auto">
                If you use CoreCognition and Concept Hacking in your research, please cite our paper
            </p>
        </div>

        <div class="max-w-4xl mx-auto">
            <div class="bg-gray-800 p-6 rounded-xl">
                <pre><code class="language-latex">@misc{li2025coreknowledgedeficitsmultimodal,
      title={Core Knowledge Deficits in Multi-Modal Language Models}, 
      author={Yijiang Li and Qingying Gao and Tianwei Zhao and Bingyang Wang and Haoran Sun and Haiyun Lyu and Dezhi Luo and Hokin Deng},
      year={2025},
      eprint={2410.10855},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.10855}, 
}</code></pre>
            </div>
        </div>
    </div>
</section>

<style>
/* Special container for paper figures with neutral background */
.paper-figure-container {
    background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
    padding: 2rem;
    border-radius: 1rem;
    border: 1px solid #e2e8f0;
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
}

.paper-figure-container img {
    background: white;
    padding: 1rem;
    border-radius: 0.5rem;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

.tab-button {
    @apply px-4 py-2 rounded-lg text-sm font-medium transition-colors;
    @apply bg-gray-200 text-gray-700 hover:bg-gray-300;
}

.tab-button.active {
    @apply bg-blue-600 text-white hover:bg-blue-700;
}

.tab-content {
    @apply transition-all duration-300;
}
</style> 

<script>
function selectDeploymentMethod(method) {
    // Remove selected state from all cards
    document.querySelectorAll('.deployment-card').forEach(card => {
        card.classList.remove('selected');
    });
    
    // Add selected state to clicked card
    document.querySelector(`[data-method="${method}"]`).classList.add('selected');
    
    // Show corresponding tab
    showTab(method);
}

function showTab(tabName) {
    // Hide all tab contents
    document.querySelectorAll('.tab-content').forEach(tab => {
        tab.classList.add('hidden');
    });
    
    // Show selected tab content
    document.getElementById(tabName + '-tab').classList.remove('hidden');
    
    // Trigger Prism.js to highlight code
    if (typeof Prism !== 'undefined') {
        Prism.highlightAll();
    }
}

// Initialize on page load
document.addEventListener('DOMContentLoaded', function() {
    // Select first deployment method by default WITHOUT scrolling
    document.querySelector(`[data-method="local"]`).classList.add('selected');
    showTab('local');
    
    // Highlight all code blocks
    if (typeof Prism !== 'undefined') {
        Prism.highlightAll();
    }
});
</script> 
